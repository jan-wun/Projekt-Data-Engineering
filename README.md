# ğŸ“¦ Projekt: Data Engineering (Bitcoin)

Dieses Projekt zeigt eine vollstÃ¤ndige datengetriebene Pipeline zur Verarbeitung, Aggregation und Visualisierung von 
Bitcoin-Marktdaten auf Quartalsbasis. Die Architektur basiert auf Microservices, die mithilfe von Docker orchestriert 
und lokal ausgefÃ¼hrt werden kÃ¶nnen.


## ğŸš€ Zielsetzung

Ziel des Projekts ist der Aufbau einer modularen, skalierbaren und wartbaren Datenpipeline mit Fokus auf:

- **Ingestion** (Apache NiFi)
- **Verarbeitung** (Apache Spark)
- **Persistenz** (PostgreSQL)
- **Visualisierung** (Streamlit)
- **Reproduzierbarkeit** (Docker, Git, .env)

---

## ğŸ§± ArchitekturÃ¼bersicht

![Data Pipeline Architecture](images/Data_Engineering_Skizze.png)

Die Pipeline besteht aus folgenden Komponenten:

| Schicht              | Komponente     | Aufgabe                                   |
|----------------------|----------------|-------------------------------------------|
| Ingestion Layer      | Apache NiFi    | Import von Rohdaten aus CSV               |
| Storage Layer        | PostgreSQL     | Speicherung von Roh-, Staging- und Aggregatdaten |
| Processing Layer     | Apache Spark   | Transformation und Aggregation            |
| Serving Layer        | Streamlit      | Interaktives Dashboard zur Datenanalyse   |
| Control & Management | Docker Compose | Orchestrierung der Microservices          |

---

## ğŸ”§ Setup & AusfÃ¼hrung

### ğŸ”‘ Voraussetzungen

- Docker & Docker Compose
- Git

### â–¶ï¸ Start

```bash
git clone https://github.com/jan-wun/Projekt-Data-Engineering.git
cd Projekt-Data-Engineering
docker-compose up --build
```

Die AusfÃ¼hrung der Pipeline kann je nach Hardware einige Minuten in Anspruch nehmen.

Die Anwendung bzw. das Streamlit Dashboard ist dann unter [http://localhost:8501](http://localhost:8501) erreichbar.

---

## ğŸ“Š Visualisierung (Streamlit Dashboard)

Das Dashboard stellt folgende Metriken quartalsweise dar:

- ğŸ“ˆ Durchschnittlicher Open-/Close-Preis
- ğŸ’µ Gesamtvolumen (total trading volume)
- ğŸ“‰ Min Low / Max High Preis

FÃ¼r das letzte vollstÃ¤ndige Quartal sind die Metriken zudem Ã¼bersichtlich aufgelistet.

![Screenshot Streamlit_1](images/streamline_dashboard_1.png)
![Screenshot Streamlit_2](images/streamline_dashboard_2.png)

---

## ğŸ—ƒï¸ Datenverarbeitung

- Die Pipeline liest Rohdaten im CSV-Format via Apache NiFi ein.
- Spark transformiert die Daten in zwei Stufen:
  - **Staging**: Umwandlung von Unix-Timestamps in `datetime`, Validierung, Bereinigung
  - **Aggregation**: Gruppierung nach Quartal mit berechneten Metriken (avg, min, max, sum)

---

## âœ… Features

- â›“ï¸ Automatische AbhÃ¤ngigkeitserkennung mit `wait-for-table.sh`
- ğŸ” Wiederholbare Verarbeitung dank Containerisierung
- ğŸ”’ Zugriffsschutz fÃ¼r sensible Daten Ã¼ber `.env`
- ğŸ§ª Robuste Validierung & Fehlerbehandlung

---

## ğŸ› ï¸ Weiterentwicklung

MÃ¶gliche Erweiterungen:

- Echtzeitdaten via Kafka
- Monitoring mit Grafana/Prometheus
- Logging mit ELK Stack
- Cloud-Deployment (z. B. AWS, Azure, GCP)

---

## ğŸ“ Projektstruktur (Kurzform)

```
â”œâ”€â”€ data/
â”‚   â””â”€â”€ btcusd_1-min_data.csv
â”œâ”€â”€ drivers/
â”‚   â””â”€â”€ postgresql-42.7.3.jar
â”œâ”€â”€ nifi/
â”‚   â””â”€â”€ Dockerfile
â”‚   â””â”€â”€ flow.xml.gz
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ transform_bitcoin_data.py
â”‚   â””â”€â”€ wait-for-table.sh
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ init_postgres.sql
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ init.sql
â”œâ”€â”€ streamlit/
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ wait-for-table.sh
â”œâ”€â”€ .env
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md

```

---

## ğŸ‘¤ Autor

**Jan Wunderlich**
